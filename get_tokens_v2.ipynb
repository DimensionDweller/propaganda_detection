{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The next transmission could be more pronounced or stronger', 'when (the plague) comes again it starts from more stock, and the magnitude in the next transmission could be higher than the one that we saw', 'appeared', 'a very, very different', 'He also pointed to the presence of the pneumonic version, which spreads more easily and is more virulent, in the latest outbreak', 'but warned that the danger was not over', 'it could even spill over into neighbouring countries and beyond']\n"
     ]
    }
   ],
   "source": [
    "def read_article(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def read_labels(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    labels = [(int(line.split('\\t')[1]), int(line.split('\\t')[2])) for line in lines]\n",
    "    return labels\n",
    "\n",
    "def get_propaganda_spans(article, labels):\n",
    "    propaganda_spans = [article[start:end] for start, end in labels]\n",
    "    return propaganda_spans\n",
    "\n",
    "# Example usage:\n",
    "article_path = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-articles/article111111111.txt'\n",
    "labels_path = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-labels-task1-span-identification/article111111111.task1-SI.labels'\n",
    "\n",
    "article = read_article(article_path)\n",
    "labels = read_labels(labels_path)\n",
    "propaganda_spans = get_propaganda_spans(article, labels)\n",
    "\n",
    "print(propaganda_spans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PropagandaDataset(Dataset):\n",
    "    def __init__(self, articles, spans):\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.articles = articles\n",
    "        self.spans = spans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles[idx]\n",
    "        labels = [0] * len(article)\n",
    "        for start, end in self.spans[idx]:\n",
    "            labels[start:end] = [1] * (end - start)\n",
    "        tokens = self.tokenizer(article, truncation=True, padding='max_length', max_length=512)\n",
    "        input_ids = tokens.input_ids\n",
    "        attention_mask = tokens.attention_mask\n",
    "        label_alignment = self.tokenizer.get_offset_mapping()\n",
    "        aligned_labels = [-100] * len(label_alignment)  # Ignore special tokens\n",
    "        for i, (start, end) in enumerate(label_alignment):\n",
    "            if start != end:  # Ignore special tokens\n",
    "                aligned_labels[i] = labels[start]\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': aligned_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PropagandaDataset(Dataset):\n",
    "    def __init__(self, articles, spans):\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.articles = articles\n",
    "        self.spans = spans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "\n",
    "        article = self.articles[idx]\n",
    "        labels = [0] * len(article)\n",
    "        for start, end in self.spans[idx]:\n",
    "            labels[start:end] = [1] * (end - start)\n",
    "        tokens = self.tokenizer(article, truncation=True, padding='max_length', max_length=512)\n",
    "        input_ids = tokens.input_ids\n",
    "        attention_mask = tokens.attention_mask\n",
    "        label_alignment = self.tokenizer.get_offset_mapping()\n",
    "        aligned_labels = [-100] * len(label_alignment)  # Ignore special tokens\n",
    "        for i, (start, end) in enumerate(label_alignment):\n",
    "            if start != end:  # Ignore special tokens\n",
    "                aligned_labels[i] = labels[start]\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': aligned_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_all_articles(directory):\n",
    "    articles = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = os.path.join(directory, filename)\n",
    "            articles.append(read_article(path))\n",
    "    return articles\n",
    "\n",
    "def read_all_labels(directory):\n",
    "    all_labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".labels\"):\n",
    "            path = os.path.join(directory, filename)\n",
    "            all_labels.append(read_labels(path))\n",
    "    return all_labels\n",
    "\n",
    "# Example usage:\n",
    "articles_directory = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-articles'\n",
    "labels_directory = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-labels-task1-span-identification'\n",
    "\n",
    "articles = read_all_articles(articles_directory)\n",
    "all_labels = read_all_labels(labels_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import os\n",
    "\n",
    "def read_all_articles(directory):\n",
    "    articles = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = os.path.join(directory, filename)\n",
    "            articles.append(read_article(path))\n",
    "    return articles\n",
    "\n",
    "def read_all_labels(directory):\n",
    "    all_labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".labels\"):\n",
    "            path = os.path.join(directory, filename)\n",
    "            all_labels.append(read_labels(path))\n",
    "    return all_labels\n",
    "\n",
    "# Example usage:\n",
    "articles_directory = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-articles'\n",
    "labels_directory = '/Users/parkermoesta/Library/Mobile Documents/com~apple~CloudDocs/github projects/propaganda_detection/propaganda_detection/datasets/train-labels-task1-span-identification'\n",
    "\n",
    "articles = read_all_articles(articles_directory)\n",
    "all_labels = read_all_labels(labels_directory)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_spans():\n",
    "    \n",
    "\n",
    "\n",
    "# Process each article\n",
    "for idx in range(len(articles)):\n",
    "    article = articles[idx]\n",
    "    labels = [0] * len(article)\n",
    "    for start, end in spans[idx]:\n",
    "        labels[start:end] = [1] * (end - start)\n",
    "    tokens = tokenizer(article, truncation=True, padding='max_length', max_length=512)\n",
    "    input_ids = tokens.input_ids\n",
    "    attention_mask = tokens.attention_mask\n",
    "    label_alignment = tokenizer.get_offset_mapping()\n",
    "    aligned_labels = [-100] * len(label_alignment)  # Ignore special tokens\n",
    "    for i, (start, end) in enumerate(label_alignment):\n",
    "        if start != end:  # Ignore special tokens\n",
    "            aligned_labels[i] = labels[start]\n",
    "    # Now you have input_ids, attention_mask, and aligned_labels for each article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_alignment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabel_alignment\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_alignment' is not defined"
     ]
    }
   ],
   "source": [
    "label_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
